# -*- coding: utf-8 -*-
"""Copy of Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lwf6Mp9mU9rzfiLQ8R5V-wlCT66KbpHP
"""

!pip install transformers datasets torch scikit-learn bertopic
!pip install datasets --upgrade

from datasets import load_dataset

dataset = load_dataset("mrm8488/goemotions")
print(dataset)

# Supposons que vous avez une liste des colonnes inutiles √† supprimer
columns_to_remove = [
    'id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc',
    'rater_id', 'example_very_unclear'
]
# Supprimer les colonnes inutiles du dataset
dataset = dataset.remove_columns(columns_to_remove)

# V√©rifier que seules les colonnes pertinentes restent
print(dataset['train'].column_names)

import re

def clean_text(text):
    # Enlever les URLs, mentions, hashtags
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#\w+', '', text)
    text = text.strip().lower()  # Convertir en minuscule et enlever les espaces inutiles
    return text

# Appliquer le nettoyage sur les textes du dataset
dataset = dataset.map(lambda x: {'text': clean_text(x['text'])})

# V√©rifier le texte nettoy√©
print(dataset['train'][0]['text'])

from transformers import RobertaTokenizer

# Charger le tokenizer pr√©-entra√Æn√© RoBERTa
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

# Fonction de tokenisation
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)

# Appliquer la tokenisation au dataset
dataset = dataset.map(tokenize_function, batched=True)

# V√©rifier un exemple apr√®s la tokenisation
print(dataset['train'][0])

from transformers import RobertaForSequenceClassification
import torch

# Cr√©er un mod√®le pr√©-entra√Æn√© pour la classification des √©motions
model_emotion = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=27)

# Exemple d'entr√©e pour tester le mod√®le
input_ids = torch.tensor([[0, 6025, 177, 2581]])  # Exemple d'input_id
attention_mask = torch.tensor([[1, 1, 1, 1]])     # Exemple d'attention_mask

# Passer les inputs dans le mod√®le pr√©-entra√Æn√© pour pr√©dire les √©motions
outputs = model_emotion(input_ids=input_ids, attention_mask=attention_mask)
logits = outputs.logits  # logits de sortie

# Afficher les logits
print(logits)

from transformers import RobertaForSequenceClassification

# Cr√©er un mod√®le pr√©-entra√Æn√© pour la classification du sentiment
model_sentiment = RobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment", num_labels=3)

# Exemple d'entr√©e pour tester le mod√®le
input_ids = torch.tensor([[0, 6025, 177, 2581]])  # Exemple d'input_id
attention_mask = torch.tensor([[1, 1, 1, 1]])     # Exemple d'attention_mask

# Passer les inputs dans le mod√®le pr√©-entra√Æn√© pour pr√©dire le sentiment
outputs = model_sentiment(input_ids=input_ids, attention_mask=attention_mask)
logits = outputs.logits  # logits de sortie

# Afficher les logits
print(logits)

!pip install bertopic

from bertopic import BERTopic

# Extract topics from the GoEmotions dataset
texts = dataset['train']['text']

# Initialize BERTopic
topic_model = BERTopic(language='english')

# Fit the model to extract topics
topics, probabilities = topic_model.fit_transform(texts)

# View the extracted topics
print(topic_model.get_topic_info())  # Topic info and frequencies
print(topic_model.get_topics())      # Words that define each topic

import glob
from pathlib import Path

# Exemple pour peupler train_imgs avec les chemins des images
train_imgs = list(Path('path/to/train/images').glob('*.jpg'))

# V√©rifiez le contenu de train_imgs
print(f"Nombre d'images dans train_imgs : {len(train_imgs)}")

"""# **Installation des d√©pendances**"""

pip install transformers torch

"""# **T√©l√©chargement du mod√®le et du tokenizer**"""

from transformers import RobertaForSequenceClassification, RobertaTokenizer
import torch

# Charger le mod√®le pr√©-entra√Æn√© pour la d√©tection du sarcasme (7 classes initiales)
model_sarcasm = RobertaForSequenceClassification.from_pretrained("j-hartmann/emotion-english-distilroberta-base", num_labels=7)

# R√©initialiser la couche de sortie pour 2 classes (sarcastique et non sarcastique)
model_sarcasm.num_labels = 2
model_sarcasm.classifier = torch.nn.Linear(model_sarcasm.config.hidden_size, 2)

# Charger le tokenizer
tokenizer_sarcasm = RobertaTokenizer.from_pretrained("j-hartmann/emotion-english-distilroberta-base")

"""# **Inference de sarcasme avec RoBERTa**"""

# Charger le mod√®le et le tokenizer pr√©-entra√Æn√©s
model_sarcasm = RobertaForSequenceClassification.from_pretrained("j-hartmann/emotion-english-distilroberta-base", num_labels=2)
tokenizer_sarcasm = RobertaTokenizer.from_pretrained("j-hartmann/emotion-english-distilroberta-base")

# Exemple de texte pour la d√©tection du sarcasme
text = "Oh great, another Monday!"  # Exemple sarcastique

# Tokeniser l'entr√©e
inputs = tokenizer_sarcasm(text, return_tensors="pt", padding=True, truncation=True, max_length=128)

# Faire des pr√©dictions sans mettre √† jour les gradients
with torch.no_grad():
    outputs = model_sarcasm(**inputs)
    logits = outputs.logits  # R√©cup√©rer les logits de sortie

# Obtenir la pr√©diction de sarcasme (1 = sarcastique, 0 = non sarcastique)
prediction = logits.argmax(dim=-1)
print(f"Predicted sarcasm: {'Sarcastic' if prediction.item() == 1 else 'Not Sarcastic'}")

"""# **Traitement des pr√©dictions et gestion des r√©sultats**"""

model_sarcasm = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=2)
tokenizer_sarcasm = RobertaTokenizer.from_pretrained("roberta-base")

pip install --upgrade torch transformers

!pip install bertopic

# Installer Hugging Face Datasets
!pip install datasets

from datasets import load_dataset

# Charger le dataset GoEmotions
dataset = load_dataset("go_emotions")

# V√©rifier les premi√®res entr√©es
print(dataset['train'][0])

from bertopic import BERTopic

# Extraire les textes du dataset
texts = dataset['train']['text']  # Assurez-vous que les noms des colonnes sont corrects

# Initialiser BERTopic
topic_model = BERTopic()

# Extraire les sujets
topics, probs = topic_model.fit_transform(texts)

# Afficher les sujets extraits

from bertopic import BERTopic

# Extract topics from the GoEmotions dataset
texts = dataset['train']['text']

# Initialize BERTopic
topic_model = BERTopic(language='english')

# Fit the model to extract topics
topics, probabilities = topic_model.fit_transform(texts)

# View the extracted topics
print(topic_model.get_topic_info())  # Topic info and frequencies
print(topic_model.get_topics())      # Words that define each topic

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(p):
    predictions, labels = p
    preds = predictions.argmax(axis=-1)
    accuracy = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average='weighted')
    return {"accuracy": accuracy, "f1": f1}

from transformers import RobertaForSequenceClassification, RobertaTokenizer
import torch

# Charger le mod√®le RoBERTa sans charger les poids de la couche finale
model = RobertaForSequenceClassification.from_pretrained("j-hartmann/emotion-english-distilroberta-base", num_labels=28, ignore_mismatched_sizes=True)

# Remplacer la couche de sortie (pour avoir 28 classes)
model.classifier = torch.nn.Linear(model.config.hidden_size, 28)  # Adapter la couche de sortie √† 28 classes

# Charger le tokenizer
tokenizer = RobertaTokenizer.from_pretrained("j-hartmann/emotion-english-distilroberta-base")

from transformers import RobertaForSequenceClassification, RobertaTokenizer
import torch

# Charger un mod√®le public (roberta-base)
model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=28)
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

# Exemple d'entr√©e pour tester
text = "I am feeling very excited today!"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)

# Passer le texte dans le mod√®le pour obtenir les pr√©dictions
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits  # Logits de sortie (scores pour chaque classe)
    predicted_class = logits.argmax(dim=-1)  # Trouver la classe avec le score le plus √©lev√©

# Liste des √©motions possibles (28 classes d'√©motions)
emotion_labels = [
    "admiration", "amusement", "anger", "annoyance", "approval", "caring",
    "confusion", "curiosity", "desire", "disappointment", "disgust", "embarrassment",
    "excitement", "fear", "gratitude", "happiness", "hate", "joy", "love",
    "nervousness", "optimism", "pride", "realization", "relief", "remorse",
    "sadness", "surprise", "fear"
]

# Afficher l'√©motion pr√©dite
predicted_emotion = emotion_labels[predicted_class.item()]
print(f"Emotion pr√©dite : {predicted_emotion}")

# V√©rifie les valeurs NaN ou non-string dans la colonne 'text'
print(df["text"].isnull().sum())  # Affiche le nombre de valeurs manquantes
print(df["text"].apply(type).value_counts())  # Affiche les types des donn√©es dans la colonne

import pandas as pd

# Charger le dataset (remplace par ton chemin de fichier)
csv_path = "/content/sentiment-analysis-dataset/train.csv"
df = pd.read_csv(csv_path, encoding='ISO-8859-1')  # Assure-toi que le chemin est correct

# V√©rifier les premi√®res lignes
print(df.head())

# Remplacer les valeurs NaN par une cha√Æne vide
df["text"] = df["text"].fillna("")

# V√©rifier les types de donn√©es apr√®s remplacement
print(df["text"].apply(type).value_counts())

# Appliquer la pr√©diction uniquement sur les valeurs de type str
df["predicted_sentiment"] = df["text"].apply(lambda x: predict_sentiment(x) if isinstance(x, str) else "unknown")

# Afficher les r√©sultats
print(df.head())

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

csv_path = "/content/sentiment-analysis-dataset/train.csv"

# Charger le dataset
df = pd.read_csv(csv_path, encoding='ISO-8859-1')
print("Rows in dataset:", len(df))
print(df.head())

# ---- Mod√®le de sentiment public (pas besoin de compte Hugging Face) ----
model_name = "cardiffnlp/twitter-roberta-base-sentiment"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Labels selon ce mod√®le
sentiment_labels = ["negative", "neutral", "positive"]

# Fonction de pr√©diction
def predict_sentiment(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=128
    )
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        prediction = torch.argmax(logits, dim=-1).item()
    return sentiment_labels[prediction]

# Faire les pr√©dictions pour tout le dataset
df["predicted_sentiment"] = df["text"].apply(predict_sentiment)

# Afficher quelques r√©sultats
print(df[["text", "predicted_sentiment"]].head())

# Sauvegarder si n√©cessaire
df.to_csv("kaggle_sentiment_predictions.csv", index=False)
print(" Sauvegard√© dans kaggle_sentiment_predictions.csv")

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Charger le mod√®le pr√©-entra√Æn√© et le tokenizer
model_name = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Liste des √©motions possibles pour le mod√®le (3 classes : 'negative', 'neutral', 'positive')
sentiment_labels = ["negative", "neutral", "positive"]

# Fonction de pr√©diction des √©motions
def predict_sentiment(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=128
    )
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        prediction = torch.argmax(logits, dim=-1).item()
    return sentiment_labels[prediction]

# Interface chatbot
def chatbot():
    print("ü§ñ Chatbot de reconnaissance des √©motions !")
    print("Tapez 'exit' pour quitter.\n")

    while True:
        # Demander une phrase √† l'utilisateur
        user_input = input("Vous: ")

        # Si l'utilisateur tape "exit", on quitte le chatbot
        if user_input.lower() == "exit":
            print("ü§ñ Au revoir !")
            break

        # Pr√©dire l'√©motion de la phrase
        predicted_emotion = predict_sentiment(user_input)

        # Afficher la r√©ponse
        print(f"ü§ñ Emotion pr√©dite : {predicted_emotion}\n")

# Lancer le chatbot
chatbot()